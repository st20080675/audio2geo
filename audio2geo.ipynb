{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OBnOLX7LfJF07GgWaP3m425jtzvA9kPp",
      "authorship_tag": "ABX9TyOFcbPh7aWXn3pWqckevH5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/st20080675/Agnes/blob/master/audio2geo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSmJTBU0U4OX",
        "outputId": "bb6457b0-2cc4-45a5-dd0c-40c40aa69f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 155184])\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "\n",
        "\n",
        "def get_sample(path, resample=None):\n",
        "  effects = [\n",
        "    [\"remix\", \"1\"]\n",
        "  ]\n",
        "  if resample:\n",
        "    effects.extend([\n",
        "      [\"lowpass\", f\"{resample // 2}\"],\n",
        "      [\"rate\", f'{resample}'],\n",
        "    ])\n",
        "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "# waveform, sample_rate = torchaudio.load('/home/LG/input.wav', normalize=True)\n",
        "waveform, sample_rate = get_sample('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train/seq_0/input.wav')\n",
        "print(waveform.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "import os\n",
        "from numpy import load\n",
        "import torch\n",
        "\n",
        "def load_sequence(path):\n",
        "  targets = None\n",
        "  audo_path = os.path.join(path, 'input.wav')\n",
        "  waveform, sample_rate = get_sample(audo_path)\n",
        "  tar_path = os.path.join(path, 'target')\n",
        "  tar_list = listdir(tar_path)\n",
        "  for i in tar_list:\n",
        "    p = os.path.join(tar_path, i)\n",
        "    x = load(p)\n",
        "    if targets == None:\n",
        "      targets = torch.from_numpy(x['values'].reshape(-1))\n",
        "      targets = targets[None, :]\n",
        "    else:\n",
        "      y = torch.from_numpy(x['values'].reshape(-1))\n",
        "      y = y[None, :]\n",
        "      targets = torch.cat((targets, y), 0)\n",
        "\n",
        "  return waveform, sample_rate, targets\n",
        "\n",
        "\n",
        "waveform, sample_rate, targets = load_sequence('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train/seq_0')\n",
        "print(waveform.shape)\n",
        "print(sample_rate)\n",
        "print(targets.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDuM0iCV9QAk",
        "outputId": "6c49892b-d6b2-4197-c383-8706b1f37fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 155184])\n",
            "48000\n",
            "torch.Size([97, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mfcc_extractor(waveform, sample_rate, desired_rate, n_mfcc):\n",
        "  frame_length = 1/desired_rate  # Frame length in seconds\n",
        "  hop_length = int(sample_rate * frame_length)  # Determine hop length based on desired frame length\n",
        "  # win_length = int(sample_rate * frame_length)  # Determine window length based on desired frame length\n",
        "  mfcc_transform = transforms.MFCC(\n",
        "      sample_rate=sample_rate,\n",
        "      n_mfcc=n_mfcc,\n",
        "      melkwargs={\n",
        "        'hop_length': hop_length,\n",
        "      }\n",
        "  )\n",
        "  mfcc = mfcc_transform(waveform)\n",
        "  return mfcc\n",
        "\n",
        "x = mfcc_extractor(waveform, sample_rate, 30, 50)\n",
        "print(x.shape)\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1I8vOMtWy9i",
        "outputId": "a5330c94-18aa-47eb-b736-dff70b307476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 97])\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os import listdir\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class MfccDataset(Dataset):\n",
        "  def __init__(self, path, desired_rate = 30, n_mfcc = 50):\n",
        "    self.n_mfcc = n_mfcc\n",
        "    self.desired_rate = desired_rate\n",
        "\n",
        "    # load all sequences\n",
        "    seq_names = listdir(path)\n",
        "    self.data = None\n",
        "    self.labels = None\n",
        "    for seq in seq_names[:3]:\n",
        "      seq_path = os.path.join(path, seq)\n",
        "      waveform, sample_rate, targets = load_sequence(seq_path)\n",
        "      # nan handle\n",
        "      waveform = torch.nan_to_num(waveform)\n",
        "      targets = torch.nan_to_num(targets)\n",
        "\n",
        "      mfcc = mfcc_extractor(waveform, sample_rate, self.desired_rate, n_mfcc)\n",
        "      mfcc = torch.squeeze(mfcc)\n",
        "      mfcc = torch.swapaxes(mfcc, 0, 1)\n",
        "\n",
        "       # trim data number\n",
        "      num = min(mfcc.shape[0], targets.shape[0])\n",
        "      mfcc = mfcc[:num]\n",
        "      targets = targets[:num]\n",
        "\n",
        "      if self.data == None:\n",
        "        self.data = mfcc\n",
        "        self.labels = targets\n",
        "\n",
        "      else:\n",
        "        self.data = torch.cat((self.data, mfcc), 0)\n",
        "        self.labels = torch.cat((self.labels, targets), 0)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index], self.labels[index]\n",
        "\n",
        "mydataset = MfccDataset('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train')\n",
        "print(mydataset.data.shape)\n",
        "print(mydataset.labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be2fxLX4q3yS",
        "outputId": "f9aa0c45-9802-41a8-d700-ea42fe60fab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([335, 50])\n",
            "torch.Size([335, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size=64, output_size=24):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _ = self.lstm(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "data = torch.randn((2, 4, 40))\n",
        "model = LSTMModel(40, 40, 24)\n",
        "x = model(data)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LykyOXzQS_op",
        "outputId": "20a91282-193d-49bc-cd27-a1cfebd44f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "shuffle = False\n",
        "dataset = MfccDataset('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyy3YWWL3NKe",
        "outputId": "01b85b9a-c0e9-443f-c2a0-8e0befcf2045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  loss_buf = []\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    y_pred = model(X_batch)\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_rmse = np.sqrt(loss.detach().numpy())\n",
        "    loss_buf.append(train_rmse)\n",
        "\n",
        "  return loss_buf\n",
        "\n"
      ],
      "metadata": {
        "id": "rLbGwk0fAZF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, eval_loader, criterion):\n",
        "  model.eval()\n",
        "  loss_buf = []\n",
        "  for X_batch, y_batch in eval_loader:\n",
        "    y_pred = model(X_batch)\n",
        "    loss = criterion(y_pred, y_batch)\n",
        "\n",
        "    eval_loss = np.sqrt(loss.detach().numpy())\n",
        "    loss_buf.append(eval_loss)\n",
        "\n",
        "  return sum(loss_buf)/len(loss_buf)"
      ],
      "metadata": {
        "id": "jquZhvBIDNBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model = LSTMModel(50)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  loss_buf = train(model, dataloader, loss_fn, optimizer)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    train_loss = sum(loss_buf)/len(loss_buf)\n",
        "    print(\"Epoch %d: train RMSE %.4f\" % (epoch, train_loss))\n",
        "    # eval_loss = eval(model, dataloader, loss_fn)\n",
        "    # print(\"Epoch %d: eval RMSE %.4f\" % (epoch, eval_loss))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04gh0l2E39k2",
        "outputId": "663531f1-f8fc-44c4-db16-419d55c8e002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train RMSE 31.1864\n",
            "Epoch 10: train RMSE 13.0288\n",
            "Epoch 20: train RMSE 8.3341\n",
            "Epoch 30: train RMSE 8.0338\n",
            "Epoch 40: train RMSE 7.8078\n",
            "Epoch 50: train RMSE 7.7629\n",
            "Epoch 60: train RMSE 7.6870\n",
            "Epoch 70: train RMSE 7.5449\n",
            "Epoch 80: train RMSE 7.5008\n",
            "Epoch 90: train RMSE 7.4618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(DepthwiseSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class MobileNet(nn.Module):\n",
        "    def __init__(self, output_dim=24):\n",
        "        super(MobileNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = DepthwiseSeparableConv(32, 64)\n",
        "        self.conv3 = DepthwiseSeparableConv(64, 128, stride=2)\n",
        "        self.conv4 = DepthwiseSeparableConv(128, 128)\n",
        "        self.conv5 = DepthwiseSeparableConv(128, 256, stride=2)\n",
        "        self.conv6 = DepthwiseSeparableConv(256, 256)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MobileNet-like model\n",
        "model = MobileNet(output_dim=24)\n",
        "\n",
        "# Print the model summary\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DvngBalH4im",
        "outputId": "d03f1387-0d94-4747-a641-2d696227c1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv2): DepthwiseSeparableConv(\n",
            "    (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
            "    (pointwise): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv3): DepthwiseSeparableConv(\n",
            "    (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)\n",
            "    (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv4): DepthwiseSeparableConv(\n",
            "    (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
            "    (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv5): DepthwiseSeparableConv(\n",
            "    (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
            "    (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (conv6): DepthwiseSeparableConv(\n",
            "    (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
            "    (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=256, out_features=24, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def Mfcc2img(mfcc, img_w):\n",
        "  num = mfcc.shape[0] // img_w\n",
        "\n",
        "  mfcc_img = mfcc[:img_w, :]\n",
        "  mfcc_img = mfcc_img[None, None, :]\n",
        "\n",
        "  for i in range(1, num):\n",
        "    x = mfcc[i*img_w:(i+1)*img_w, :]\n",
        "    x = x[None, None, :]\n",
        "    mfcc_img = torch.cat((mfcc_img, x), 0)\n",
        "\n",
        "  return mfcc_img"
      ],
      "metadata": {
        "id": "bOfGv49BLGDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os import listdir\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class MfccImageDataset(Dataset):\n",
        "  def __init__(self, path, desired_rate = 30, img_w = 100, n_mfcc = 50):\n",
        "    self.n_mfcc = n_mfcc\n",
        "    self.desired_rate = desired_rate\n",
        "    self.img_w = img_w\n",
        "\n",
        "    # load all sequences\n",
        "    seq_names = listdir(path)\n",
        "    self.data = None\n",
        "    self.labels = None\n",
        "    for seq in seq_names[:3]:\n",
        "      seq_path = os.path.join(path, seq)\n",
        "      waveform, sample_rate, targets = load_sequence(seq_path)\n",
        "      # nan handle\n",
        "      waveform = torch.nan_to_num(waveform)\n",
        "      targets = torch.nan_to_num(targets)\n",
        "\n",
        "      mfcc = mfcc_extractor(waveform, sample_rate, self.desired_rate * self.img_w, n_mfcc)\n",
        "      mfcc = torch.squeeze(mfcc)\n",
        "      mfcc = torch.swapaxes(mfcc, 0, 1)\n",
        "      mfcc_img = Mfcc2img(mfcc, self.img_w)\n",
        "\n",
        "      # trim data number\n",
        "      num = min(mfcc_img.shape[0], targets.shape[0])\n",
        "      mfcc_img = mfcc_img[:num]\n",
        "      targets = targets[:num]\n",
        "\n",
        "      if self.data == None:\n",
        "        self.data = mfcc_img\n",
        "        self.labels = targets\n",
        "\n",
        "      else:\n",
        "        self.data = torch.cat((self.data, mfcc_img), 0)\n",
        "        self.labels = torch.cat((self.labels, targets), 0)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.data[index], self.labels[index]\n",
        "\n",
        "mydataset = MfccImageDataset('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train')\n",
        "print(mydataset.data.shape)\n",
        "print(mydataset.labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdEEtGk4IYR9",
        "outputId": "21f71de8-8ce5-404b-c3f9-eea2d2ad0870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([333, 1, 100, 50])\n",
            "torch.Size([333, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 4\n",
        "shuffle = False\n",
        "dataset = MfccImageDataset('/content/drive/MyDrive/audio_to_geometry_homework_v2/audio_to_geometry_homework/train', desired_rate=30)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "import numpy as np\n",
        "model = MobileNet()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  loss_buf = train(model, dataloader, loss_fn, optimizer)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    train_loss = sum(loss_buf)/len(loss_buf)\n",
        "    print(\"Epoch %d: train RMSE %.4f\" % (epoch, train_loss))\n",
        "    # eval_loss = eval(model, dataloader, loss_fn)\n",
        "    # print(\"Epoch %d: eval RMSE %.4f\" % (epoch, eval_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qZR8JDfOlwd",
        "outputId": "6f9177e4-ca36-470c-b20f-2a45ad6170a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train RMSE 14.4361\n",
            "Epoch 10: train RMSE 7.2290\n",
            "Epoch 20: train RMSE 7.0743\n",
            "Epoch 30: train RMSE 7.0160\n",
            "Epoch 40: train RMSE 6.9231\n",
            "Epoch 50: train RMSE 6.9016\n",
            "Epoch 60: train RMSE 6.7321\n",
            "Epoch 70: train RMSE 6.2551\n",
            "Epoch 80: train RMSE 5.7540\n",
            "Epoch 90: train RMSE 5.1883\n"
          ]
        }
      ]
    }
  ]
}